// HTML 파일에서 아래와 같이 두 개의 태그를 준비합니다.
<!-- 웹 카메라 화면을 보여줄 비디오 태그 -->
<video id="videoElement" width="400" height="300" autoplay></video>
<!-- 감정 분석 결과를 출력할 요소 -->
<div id="resultElement"></div>

// JavaScript 코드
// 웹 카메라 화면을 보여줄 비디오 요소와 감정 분석 결과를 출력할 요소를 가져옵니다.
const videoElement = document.getElementById('videoElement');
const resultElement = document.getElementById('resultElement');

// getUserMedia() 함수를 사용하여 웹 카메라에 접근합니다.
navigator.mediaDevices.getUserMedia({ video: true })
.then((stream) => {
  // 비디오 요소에 카메라 스트림을 설정하여 웹 카메라 화면을 보여줍니다.
  videoElement.srcObject = stream;

  // 비디오 요소의 "play" 이벤트가 발생하면, 비디오 화면의 현재 프레임을 캡처하고 base64로 인코딩하여 Vision AI API로 전송합니다.
  videoElement.addEventListener('play', () => {
    const canvas = document.createElement('canvas');
    const context = canvas.getContext('2d');

    setInterval(() => {
      context.drawImage(videoElement, 0, 0, canvas.width, canvas.height);
      const imageDataURL = canvas.toDataURL('image/jpeg');
      const base64ImageData = imageDataURL.replace(/^data:image\/(png|jpeg|jpg);base64,/, '');

      // Vision AI API에 POST 요청을 보내어 감정 분석 결과를 가져옵니다.
      fetch('https://vision.googleapis.com/v1/images:annotate?key=YOUR_API_KEY', {
        method: 'POST',
        body: JSON.stringify({
          requests: [
            {
              image: { content: base64ImageData },
              features: [{ type: 'FACE_DETECTION', maxResults: 1 }],
            },
          ],
        }),
        headers: { 'Content-Type': 'application/json' },
      })
      .then(response => response.json())
      .then(data => {
        // 감정 분석 결과를 처리하여 출력합니다.
        const emotions = data.responses[0].faceAnnotations[0].joyLikelihood;
        resultElement.textContent = `감정: ${emotions}`;
      })
      .catch(error => {
        console.error('에러 발생:', error);
      });
    }, 1000); // 1초마다 카메라 프레임을 캡처하고 감정 분석 결과를 가져옵니다.
  });
})
.catch(error => {
  console.error('카메라 접근에러 발생:', error);
  })
.catch(error => {
console.error('카메라 접근에 에러가 발생하였습니다:', error);
});
