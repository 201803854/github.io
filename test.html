<!DOCTYPE html>
<html>
<head>
    <title>Webcam Emotion Detection</title>
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
</head>
<body>
    <h1>Webcam Emotion Detection</h1>
    <div>
        <video id="videoElement" width="640" height="480" autoplay></video>
    </div>
    <div>
        <h2>감정 분석 결과</h2>
        <p id="resultElement"></p>
    </div>
    <script>
// 필요한 라이브러리를 로드합니다.
const { Storage } = require('@google-cloud/storage');
const vision = require('@google-cloud/vision');
const cv = require('opencv4nodejs');

// Google Cloud Storage 클라이언트를 생성합니다.
const storage = new Storage();
// Google Cloud Vision 클라이언트를 생성합니다.
const client = new vision.ImageAnnotatorClient();

// 카메라에서 얼굴 이미지를 촬영하고 감정 분석을 수행하는 함수
async function captureAndAnalyzeEmotion() {
  try {
    // 카메라에서 이미지를 촬영합니다.
    const image = await cv.VideoCapture(0).readAsync();
    // 이미지를 JPEG 형식으로 인코딩합니다.
    const jpegImage = cv.imencode('.jpg', image).toString('base64');
    
    // 감정 분석을 위해 Google Cloud Vision API에 이미지를 전송합니다.
    const [result] = await client.faceDetection({
      image: { content: jpegImage },
      features: { type: 'FACE_DETECTION', maxResults: 1 },
    });

    // 감정 분석 결과를 추출합니다.
    const faceAnnotations = result.faceAnnotations;
    if (faceAnnotations && faceAnnotations.length > 0) {
      const emotions = faceAnnotations[0].expressions;
      console.log('감정 분석 결과:');
      console.log(emotions);
    } else {
      console.log('얼굴이 감지되지 않았습니다.');
    }
  } catch (err) {
    console.error('오류 발생:', err);
  }
}

// 카메라에서 얼굴 이미지 촬영 및 감정 분석 수행
captureAndAnalyzeEmotion();
    </script>
</body>
</html>
